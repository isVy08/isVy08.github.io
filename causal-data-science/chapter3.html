<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1.0'>

  <meta name="description" content="Vy Vo">
  <meta property="og:description" content="A Handbook on Applying Causal Inference in Business" />
  <meta property="og:image" content="https://isvy08.github.io/icon/vv.png" />
  <title>A Bigger Picture</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="../main.css">
</head>

<body>

  <div class="wrapper-masthead">
    <div class="container">
      <header class="masthead clearfix">
        <a href="https://isvy08.github.io/" class="site-avatar"><img src="../icon/vv.png" /></a>

        <div class="site-info">
          <h1 class="site-name"><a href="https://isvy08.github.io/causal-data-science/">Causal Data Science: A Beginner
              Guide</a></h1>
          <p class="site-description">A Handbook on Causal Inference in Business</p>
        </div>

      </header>
      <br>
      <div class="topnav">
        <a href="https://isvy08.github.io/causal-data-science/">About</a>
        <a class="active">Fundamentals</a>
        <a href="#">&#128274 Experimentation </a>
        <a href="#">&#128274 Applications</a>
        <a href="#">&#128274 Tools</a>
      </div>
    </div>
  </div>


  <div id="main" role="main" class="container">
    <h2 id="2">Chapter 3. A Bigger Picture</h2>
    <article class="post" style="text-align: justify;">

      <div class="toc">
        <h5 style="text-decoration: underline">Table of Contents</h5>
        <ul>
          <a href="#3.1">
            <li>3.1. Putting It All Together</li>
          </a>
          <a href="#3.2">
            <li>3.2. \(D.I.E\) Flowchart</li>
          </a>
          <a href="#3.3">
            <li>3.3. Case Study: Job Training Partnership Act</li>
          </a>
          <a href="#3.4">
            <li>3.4. Pearl's Causal Reasoning Hierarchy</li>
          </a>
        </ul>
      </div>
      <h3 id="3.1">3.1. Putting It All Together</h3>
      <p>
        A <strong>causal model</strong> consists of a <strong>directed acyclic graph (DAG)</strong> over a set of random
        variables in which
        each variable is a node, and depedencies among variables are encoded in the edges.
        If there exists a directed edge pointing from a variable \(X\) to another variable \(Y\), we say \(X\) is a
        parent and a direct cause of \(Y\).
        A system of \(Y\) and all its parents form a <strong>causal mechanism</strong> for \(Y\).
        In such a mechanism, the value of \(Y\) is determined by the value of its parents through a stochastic function
        \(f\).
      </p>
      <figure style="text-align: center;">
        <img src='figures/1-3-Fig1.png' alt='graphs' width="380" />
        <figcaption style="font-size: small;">Figure 3.1: Building Blocks of Causal Inference</figcaption>
      </figure>
      <p>
      </p>

      <p>
        <a href="chapter1.html">Chapter 1</a> introduces Bayesian Network as a toolkit to translate graphical language
        to probabilistic language.
        <strong>\(d-\)Separation</strong> is a test for graphical independency through the notion of
        <strong>blockage</strong>.
        According to <strong>Global Markov Assumption</strong>, the conditional independence yielded by a graph \(G\)
        licenses conditional independence with respect to the joint probability distribution \(P\),
        given that \(P\) adheres to the chain rule of Bayesian Network relative to \(G\).
      </p>
      <p>
        The entire paradigm is buit upon a set of assumptions that render conditions under which causal inference
        frameworks can be applied.
        This means whenever given a dataset with probability distribution \(P\) and a causal graph \(G\), we must first
        test whether \(P\) and \(G\) are compatible.
      </p>
      <p>
        We have also explained why correlation does not imply causation by examining the concept of
        <strong>Intervention</strong>.
        Statistical estimand \(P(Y|X)\) embedded in observational data is referred to <strong>associational
          distribution</strong>,
        while <strong>interventional distribution</strong> \(P(Y|do(X))\) is obtained through doing interventions on
        \(X\).
        The causal estimand \(P(Y|do(X))\) is what to be established for deriving the causal effect of \(X\) on \(Y\),
        while \(P(Y|X)\) is often a spurious signal.
      </p>
      <p>
        Doing interventions is equivalent to conducting experiments by nature, specifically controlled randomized
        experiments.
        Part II will delineate why randomization is the key to causation, and the capacity of performing large-scale
        experiments efficiently is a valuable assset that tech companies should continue to exploit.
      </p>
      <p>
        Intervention assumes the causal mechanisms are <strong>localized</strong> and <strong>independent</strong>. This
        means intervention on a variable only impacts its causal mechanism, leaving the rest of the system intact.
        The graph corresponding to intervention on a node is one constructed by deleting all arrows pointing into that
        node.
        Intervention on SCM is done by replacing the right-hand side of the structural equation with the value or
        distribution imposed by the intervention.
      </p>
      <h3 id="3.2">3.2. \(D.I.E\) Flowchart</h3>
      <h4>3.2.1. Identification</h4>
      <p>
        Unfortunately, experimentation is not always feasible while observational data is massive. This is when Causal
        Inference wields its power.
        <a href="chapter2.html">Chapter 2</a> presents fundamental techniques to identify causal effects from
        observational data with respect to a given causal model.
        The goal is to reduce causal estimands involving the \(do\) operator to statistical quantities to be computed
        from the data.
      </p>
      <p>
        Non-parametric methods are generally preferable since they do not rely on any particular functional forms.
        <a href="chapter2.html#2.1">Back-door criterion</a> is the simplest solution to blocking confounding paths,
        which is often impractical due to unobserved confounders.
        <a href="chapter2.html#2.2">Front-door criterion</a> is proposed to address this limitation. \(Do-\)calculus is
        a set of inference rules to eliminate the \(do\) operator in a general setting.
        With a specified structural equation, <a href="chapter2.html#2.6">Instrumental variable</a> can be exploited for
        identification if such a variable can be found.
        Neverthles, one should note that not all causal estimands are identifiable.
      </p>
      <h4>3.2.2. Estimation</h4>
      <p>
        So far we have tackled <strong>Identification</strong> problems. Once settling with statistical estimands, the
        next task is <strong>estimation</strong>.
        Estimation can get challenging since it normally requires controlling for multiple variables. We may also come
        across heterogeneous treatment effects - effects varying by different groups of population.
        Part II explores estimation in more detail and Part IV zooms in on popular computer-assisted methods.
      </p>
      <h4>3.2.3. Discovery</h4>
      <p>
        Given a causal model, not only are we able to determine the causal relationships among variables of interest,
        but also to estimate the causal effects quantitatively from observational data.
        In several applications, especially high-dimensional settings, the causal graph remains unknown. Thus, we
        sometimes need to infer the causal graph from observational data.
        This is the problem of <strong>Causal Discovery</strong> or <strong>Structural Learning.</strong>
      </p>

      <p>
        Causal learning algorithms from observational data can be broadly categorized into
        <strong>Constraint-based</strong> and <strong>Score-based</strong> approaches.
        The task, again, is to find the causal graph that aligns with the true data-generating process.
        Constraint-based methods uncover a set of causal graphs up to Markov equivalence class (groups of DAGs
        satisfying the same set of \(d-\)separation).
        They entail a series of conditional independence tests implemented independently with binary decisions of either
        accepting or rejecting the null hypothesis.
      </p>
      <p>
        Meanwhile, score-based approach is more efficient in that it aims to evaluate the quality of a candidate causal
        graph according to a score function i.e., graphs with higher scores are better fits.
        Thus, the function must satisfy certain properties to be considered an appropriate goodness-of-fit test.
      </p>
      <p>
        Both approaches generally assume <strong>Markov property</strong> (local at least), <strong>sufficiency</strong>
        (no unobserved confounders) and <strong>acyclicity</strong> (to be compatible with DAGs).
      </p>
      <p>
        Causal discovery is a non-trivial task that fortunately can be automated. Not only do these libraries support
        learning causal structures but they also allow for modifications based on expertise knowledge.
        Curious readers are encouraged to go through the following materials
      <ul>
        <li><a href="https://www.frontiersin.org/articles/10.3389/fgene.2019.00524/full" target="_blank">Review of
            Causal Discovery Methods Based on Graphical Models</a> (Glymour et al. 2019)</li>
        <li><a href="https://www.youtube.com/playlist?list=PLoazKTcS0Rzb6bb9L508cyJ1z-U9iWkA0">Chapter 11 & 12 -
            Introduction to Causal Inference</a> (Neal 2020)</li>
      </ul>
      </p>
      <h4>In summary</h4>
      <p>
        It is impossible to identify causal relationships without the knowledge on the data generating process. We have
        seen that observational data do not tell a complete story, thus forcing us to make assumptions along the way to
        make the most use of it.
        This urges data practitioners to pay attention to the data collection process and product usage behavior.
      </p>
      <p>
        $$Discovery \longrightarrow Identification \longrightarrow Estimation$$
      </p>

      <h3 id="3.3">3.3. Case Study: Job Training Partnership Act</h3>
      <p>
        The National JTPA Study was an initiative by the Department of Labor in the United States that aimed to measure
        the impact of a job training program on participantsâ€™ post-experimental earnings for 18 months.
        The study not only conducted randomized experiments but also collected observational data including data on
        non-participants.
      </p>
      <p>
        <a href="https://scholar.harvard.edu/files/aglynn/files/glynnkashin-frontdoor.pdf">Glynn and Kashin (2014)</a>
        first evaluated the difference in average earnings between the experimental treatment group and the
        experimental control group, segregated by gender.
        The treatment group consisted of the individuals applying to the program and
        allowed to receive JTPA training services,
        while control group consists of randomly chosen individuals prevented from accessing the services.
      </p>
      <p>
        The results of observational causal analysis are benchmarked against the experimental results.
        Observational data was obtained by selecting program participants as well as non-participants,
        according to certain eligibility criteria, to fill out a survey inquiring about participation type and other
        background information.
        Glynn and Kashin compared the efficacy of back-door and front-door estimates, and empirically proved that the
        front-door criterion is a reliable alternative to randomized experiments in the presence of unobserved
        confounders.
      </p>
      <p>
        What is interesting thing about the study is there were participants that signed up but did not show up in the
        treament group, i.e., non-compliers.
        Non-compliance is usually a nightmare in experimentation, but for such a causal analysis, it serves as a useful
        front-door criterion.
      </p>
      <p>
        Figure 3.2 illustrates the causal graph for JTPA case study

      </p>
      <figure style="text-align: center;">
        <img src='figures/1-3-Fig2.png' alt='graphs' width="400" />
        <figcaption style="font-size: small;">Figure 3.2: Causal Graph for the National JTPA Study</figcaption>
      </figure>

      <p>
        The variables are summarized as follow
      <ul>
        <li>\(Y:\) mean earnings in 18 months</li>
        <li>\(T:\) receving training (treatment) or not</li>
        <li>\(M:\) show up in training sessions (complier) or not </li>
        <li>\(X:\) other covariates such as demographics, social program participation, training and education histories
        </li>
        <li>\(U:\) unobserved or immeasurable confounding variables such as motivation</li>
      </ul>
      </p>
      <p>
        Blocking the back-door path opened up by \(X\), the back-door estimate is
        $$P^{bd}(y|do(T)=t) = \sum_x P(y|x,t)P(x) \quad (1)$$
      </p>
      <p>
        On the other hand, we can also use \(M\) as a front-door criterion.
        However, conditioning on \(M\) alone opens up the back-door path \(T \leftarrow X \rightarrow Y\).
        We therefore must condition on \(X\) as well.
        $$P^{fd}(y|do(T)=t) = \sum_x \sum_m P(m|t,x)\sum_{t'} P(y|t', m, x)P(t')P(x) \quad (2)$$
      </p>

      <p>
        Both estimates are biased, since there exists an unblocked back-door path created by \(U\).
        However, Glynn and Kashin demonstrated that, in case the effect of \(U\) on \(M\) is is sufficiently small
        or due to randomness (for example, bad weather made some participants less motivated to show up),
        the front-door estimate approximates the experimental benchmarks.
        Meanwhile, back-door adjustment turned out to give a poor estimate, and such covariates were shown not
        sufficient conditioning sets to mitigate the biases caused by unobserved confounders.
      </p>
      <figure style="text-align: center;">
        <img src='figures/1-3-Fig3.png' alt='graphs' width="850" />
        <figcaption style="font-size: small;">Figure 3.3 (taken from the paper):
          Comparison of Back-door and Front-door estimate across various conditioning sets.
          The experimental estimate is denoted as a dotted dark grey line,
          with the shaded grey region representing the 95% confidence interval.</figcaption>
      </figure>
      <h4>Causal Effect in the Treated</h4>
      <p>
        In the case study, the authors did not measure the standard causal effect as indicated in \((1)\) and \((2)\).
        In many social applications, the <strong>causal effect in the treated</strong> is of interest.
        The technique effectively assumes that treatment assignment has no effect if treatment is not received.
        Thus, we only examine the participants who signed up for the training i.e., \(T=1\).
        Denoting this group as \(T_1\), we extend \((1)\) and \((2)\) as follows
      </p>
      <h5>Back-door Adjustment for the Treated</h5>
      <p>
        $$P^{bd}(y|do(T)=t, T_1) = \sum_x P(y|x,t)P(x|T_1)$$
        An important note is the quantity \(P(y|do(T)=0, T_1)\) measures the effect on \(Y\)
        <strong>had the treated not been treated</strong>,
        which contradicts the reality in which all individuals in \(T_1\) were actually treated.
        \(P(y|do(T)=0, T_1)\) is thus said to be <strong>counterfactual</strong> and can be estimated as
        $$P^{bd}(y|do(T)=0, T_1) = \sum_x P(y|x,T=0)P(x|T=1)$$
        And the "factual" interventional distribution from \(do(T)=1\) is
        $$P^{bd}(y|do(T)=1, T_1) = P(y|T_1) = P(y|T=1)$$
      </p>
      <h5>Front-door Adjustment for the Treated</h5>
      <p>
        $$P^{fd}(y|do(T)=t, T_1) = \sum_x \sum_m P(m|t,x)\sum_{t'} P(y|t', m, x)P(t'|T_1)P(x|T_1)$$
        Because \(P(t=1|T_1) = 1\) and \(P(t=1|T_1) = 0\), the expression becomes
        $$P^{fd}(y|do(T)=t, T_1) = \sum_x \sum_m P(m|t,x) P(y|T_1, m, x)P(x|T_1)$$
        Similarly, we have
        $$P^{fd}(y|do(T)=1, T_1) = P(y|T_1) = P(y|T=1) $$
      </p>
      <p>
        Note that \(P(M=0|T=0, x)=1\) and \(P(M=1|T=0, x)=0\) i.e., the non-treated never shows up.
        The counterfactual distribution is
        $$P^{fd}(y|do(T)=0, T_1) = \sum_x P(y|T_1, M=0, x)P(x|T_1) = \sum_x P(y|T=1, M=0, x)P(x|T=1) $$
      </p>
      <p>
        Again, the quantities on the right-hand side of these equations can be easily derived out of observational data.
      </p>
      <h3 id="3.4">3.4. Pearl's Causal Reasoning Hierarchy </h3>
      <p>
        The JTPA case study exposes us to the concept of <strong>Counterfactuals</strong>.
        We have also seen that although the counterfactual terms describe non-existent or imaginary events,
        it is sometimes possible to estimate them from observational data.
        An important application of Counterfactuals is Mediation Analysis, and
        we will see more details of these concepts in the next chapter.
      </p>
      <p>
        Counterfactual fits in a bigger picture of <strong>Pearl's Causal Ladder</strong>,
        which nicely summarizes what Causality is in a nutshell.
        To reason causally is to find the answers to the following queries
      </p>
      <figure style="text-align: center;">
        <img src='figures/1-3-Fig4.png' alt='graphs' width="850" />
        <figcaption style="font-size: small;">Figure 3.5: Pearl's Causal Ladder (UCLA)</figcaption>
      </figure>



    </article>
  </div>
  <div style="text-align: center; padding-top: 20px;">
    <a href="chapter2.html" class="navbutton">&#9664 Chapter 2. Identifying Causal Effects</a>
    <a href="chapter4.html" class="navbutton">&#128274 Chapter 4. Counterfactuals</a> <!-- &#9654 -->
  </div>
  <div class="wrapper-footer">
    <div class="container">
      <footer class="footer">


        <a href="https://www.facebook.com/isVy08"><i class="svg-icon facebook"></i></a>
        <a href="https://twitter.com/isVy08"><i class="svg-icon twitter"></i></a>
        <a href="https://www.linkedin.com/in/isvy08/"><i class="svg-icon linkedin"></i></a>
        <br>
        <p style="font-size: 10px;">&copy; 2021 Vy Vo. Powered by Jekyll and Jekyll Now Theme</p>

      </footer>
    </div>
  </div>
</body>

</html>