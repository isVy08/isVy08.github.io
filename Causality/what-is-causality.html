<!DOCTYPE html>
<html>

<head>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital@0;1&display=swap" rel="stylesheet"></head>

<header style="font-size:250%">
    A Little Thinking on Causality
</header>

<body style="font-family: Lora;
            text-align:justify;
            padding-left: 5cm;
            padding-right: 5cm;
            padding-top: 1cm;
            line-height: 1.5;
            font-size: 18px;">


<header style="font-size:250%">
    A Little Thinking on Causality
</header>
<hr>
<article>

<h1>Beyond Correlation: Introduction to Causal Thinking (Part 1)</h1>
If correlation is not causation, then what is?

<h2>Causality is inevitable</h2>

<div class="p1">
    “Correlation does not imply causation” has been a long-held mantra among generations of scientists. 
    Throughout my experience as an analyst, stating cause and effect is considered a taboo. 
    We refrain from talking about causality, thus resort to association-disguised conclusions such as “X and Y are related” or “X affects Y” while “X is a cause of Y” is all what we want to substantiate. 
    It’s because it’s impossible to act on pure associations.

    <br><br>
    To be more specific, one of my projects involved figuring out ways to increase conversion rate for an e-commerce website. 
    I took as many factors into account as possible and worked out their correlations with the target variable. 
    One of the findings was that users who clicked to view at least 3 product options were more likely to end up purchasing the product. 
    In other words, there was a strong (positive) association between the number of products viewed and the likelihood of purchase. 
    This statistically and intuitively made sense but how to act on this finding was a headache. 

    <br><br>
    Does this mean that we just made users view plenty of options and sales would increase accordingly? 
    Now this intuitively sounds suspicious!
    None of us were equipped with the language of causality (I did not even know the field of causal inference actually existed), but instinct of an analyst told us that there must be some effects left unaccounted for. And we all circled back to questioning why a user viewing more products tended to convert and whether it was actually the case.
    After tons of effort of avoiding it, we succumbed to the desire to unveil the causal relationship. As this continues to happen in every single analysis I did, it hit me that no meaningful action can be taken without causal insights.

    <br><br>
    <q style="font-style: italic;">One cannot substantiate causal claims from associations alone, even at the population level — behind every causal conclusion there must lie some causal assumption that is not testable in observational studies.</q> — Judea Pearl (2009)
</p>

<h2>What is Causality?</h2>

<p>
    An action or a treatment X is said to have a causal effect if the outcome of when the action is taken is different from the outcome of when the action is withheld. 
    Otherwise, X has no causal effect on the outcome.
    Imagine you were constipated and had a banana. 
    The next thing you know is your bowel movement went normally. 
    To fully substantiate causality in the statement “Banana relieves constipation”, you must indulge in a parallel universe by asking “Had I not eaten that banana, would my constipation have gone away?” 
    If the answer is No, then all credit goes to the banana.
    
    <br><br>
    Let’s explain it mathematically.
    Suppose I could go back in time and had decided to conduct an experiment to justify my finding on our users’ behavior. 
    Assume I was able to restrict some users to viewing less than 3 options while allow the others to view 3 or more, and I want to figure out whether they would end up buying the product.
    
    <br><br>
    Let X be a random variables for treatment (1: view at least 3 options, 0: view under 3 options) and Y be an outcome variable (1: purchase, 0: not purchase). 
    Here we also have to strictly define the time frame under which to measure our outcome, for example purchase within 1 week after being assigned the treatment, but you can safely ignore this for now.
    <br>
        <li style="padding-left: 0.5cm; font-family: Courier New;">Y<sub>i</sub> (X = 0) is the outcome variable that would have been observed in an individual i under treatment value X = 0</li>
        <li style="padding-left: 0.5cm; font-family: Courier New;">Y<sub>i</sub> (X = 1) is the outcome variable that would have been observed in that individual under treatment value X = 1</li> 
    <br>
    For an arbitrary individual user named John, if
    
    <br>
    <p style="padding-left: 1cm; font-family: Courier New;">Y<sub>i</sub> (X = 0) &#8772 Y<sub>i</sub> (X = 0)</p>

    we say X has a causal effect on John’s outcome and not otherwise. 
    This is individual causal effect and such outcomes are referred to counterfactual outcomes.

    <br>
    For a well-defined population, we measure average causal effect. Such an effect is present in the population if

    <br>
    <p style="padding-left: 1cm; font-family: Courier New;">
        P( Y | do(X) = 1) &#8772 P( Y | do(X) = 0)
    </p>

    <span style="font-family: Courier New;">
        P( Y | do(X) = 1)
    </span> and 
    <span style="font-family: Courier New;">
        (Y | do(X) = 0)
    </span> are defined as the proportions of the population developing the outcome had all individuals received treatment X = 0 and X = 1 respectively. 
    In this case, I could have compared 
    <span style="font-family: Courier New;">
        P ( Y = 1 | do(X) = 1)
    </span> with 
    <span style="font-family: Courier New;"> 
        P ( Y = 1 | do(X) = 0)
    </span> — the proportion of users that actually purchased if all users had been forced to view at least 3 options with the proportion of those that did had they been made to view less than 3.

    <br><br>
    At this point, you may find these notations somewhat similar to those we use for conditional probabilities. 
    Indeed, 
    <span style="font-family: Courier New;">
        P (Y | X = 0)
    </span> and 
    <span style="font-family: Courier New;">
        P (Y | X = 1)
    </span> are associational probabilities derived from data collected through Big Data systems or generally from observational studies. Recall that 
    <span style="font-family: Courier New;">
        P (Y = 1 | X = 0)
    </span> is the proportion of users that purchased given that they have viewed less than 3 options.

    <br><br>
    The two concepts are fundamentally different, and the discrepancy lies in the <strong>do</strong> operator. 
    While association is about <strong>observation</strong>, causation is about intervention. 
    The keyword here is to <strong>force</strong>. 
    If any causal effect was found my experiment, it would be due to my intervention in which I subjected the users to one of the 2 treatments. 
    Recall that the correlation was found within my data whereas my hypothetical experiment seeks to answer the question “Would they have converted had I made them view at least 3 options.” 
    
    <br><br>
    <strong>To do</strong>  and <strong>To be forced to do</strong> are not just semantically but statistically distinct.
</div>

<br><br> 
<a href="find-causality.html">Intro to Causal Thinking (Part 2)</a>

<h3>References</h3>
<ul style="font-style: italic;">
    <li >
        Causality: Models, Reasoning and Inference — Judea Pearl (2009)
    </li>
    <li>
        Causal inference in statistics: An overview — Judea Pearl (2009)
    </li>
    <li>
        The Book of Why: The New Science of Cause and Effect — Judea Pearl and Dana Mackenzie (2018)
    </li>
    <li>
        Causal Inference: What If — Miguel A. Hernán and James M. Robins (2020)
    </li>
</ul>





</article>
</body>
</html>